% !TeX program = pdflatex
\documentclass[letterpaper,journal]{IEEEtran}

% --- PACKAGES ---
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{setspace}
\usepackage[most]{tcolorbox}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{xkeyval}
\usepackage{wrapfig}

% Hyperlinks (boxed links: citations=green, internal refs (fig/table/section)=red)
\usepackage[colorlinks=false, pdfborder={0 0 1}]{hyperref}
\hypersetup{
    linkbordercolor={1 0 0},
    citebordercolor={0 1 0},
    urlbordercolor={0 0 1}
}

% --- MACROS ---
\newcommand{\modelname}{Self-Evolve-VLM} % Name placeholder


% --- Author biography helper (aligned with bare_jrnl_new_sample4.tex) ---
\makeatletter
\newif\ifAuthorbibHasWraplines
\define@key{authorbib}{scale}[1]{%
\def\AuthorbibKVMacroScale{#1}%
}
\define@key{authorbib}{wraplines}{%
\AuthorbibHasWraplinestrue
\def\AuthorbibKVMacroWraplines{#1}%
}
\define@key{authorbib}{imagewidth}[4cm]{%
\def\AuthorbibKVMacroImagewidth{#1}%
}
\define@key{authorbib}{overhang}[10pt]{%
\def\AuthorbibKVMacroOverhang{#1}%
}
\define@key{authorbib}{imagepos}[l]{%
\def\AuthorbibKVMacroImagepos{#1}%
}
\makeatother

\presetkeys{authorbib}{imagepos=l, imagewidth=4cm, overhang=20pt}{}
\newlength{\AuthorbibTopSkip}
\newlength{\AuthorbibBottomSkip}
\setlength{\AuthorbibTopSkip}{0.3\baselineskip}
\setlength{\AuthorbibBottomSkip}{0.3\baselineskip}

% Usage: \authorbibliography[<key=val,...>]{<image>}{<bio text>}
\NewDocumentCommand{\authorbibliography}{+o+m+m}{%
    \begingroup
    \AuthorbibHasWraplinesfalse
    \def\AuthorbibKVMacroWraplines{}%
    \IfNoValueTF{#1}{}{\setkeys{authorbib}{#1}}%
    \vspace{\AuthorbibTopSkip}
    \noindent\relax
    \makeatletter
    \ifAuthorbibHasWraplines
        \begin{wrapfigure}[\AuthorbibKVMacroWraplines]{\AuthorbibKVMacroImagepos}[\AuthorbibKVMacroOverhang]{\AuthorbibKVMacroImagewidth}
            \includegraphics[scale=\AuthorbibKVMacroScale]{#2}
        \end{wrapfigure}
    \else
        \begin{wrapfigure}{\AuthorbibKVMacroImagepos}[\AuthorbibKVMacroOverhang]{\AuthorbibKVMacroImagewidth}
            \includegraphics[scale=\AuthorbibKVMacroScale]{#2}
        \end{wrapfigure}
    \fi
    \makeatother
    #3
    \vspace{\AuthorbibBottomSkip}
    \par
    \endgroup
}

\begin{document}

% Reduce spacing for IEEEbiography environment
\makeatletter
\def\@IEEEBIOskipN{10pt}
\makeatother

% --- TITLE ---
\title{From Self-Rewarding to Self-Evolving: A Unified Framework for Adaptive Multimodal Reasoning}

\author{Changbin Cheng,
    Jisheng Dang,
    Bimei Wang,
    Yulan Guo,~\IEEEmembership{Senior Member,~IEEE},
    Bin Hu,~\IEEEmembership{Fellow,~IEEE},
    Chua Tat Seng

\thanks{This work was supported by the National Natural Science Foundation of China (Grants No. 62227807 and U24B20186). This work was also supported by the Supercomputing Center of Lanzhou University.}
\thanks{Changbin Cheng, Jisheng Dang, and Bimei Wang are with School of Information Science \& Engineering, Lanzhou University, Lanzhou 730000， China.  (E-mail: dangjsh@mail2.sysu.edu.cn.}


\thanks{Huicheng Zheng and Jianhuang Lai are with the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China.  (E-mail:  zhenghch@mail.sysu.edu.cn, stsljh@mail.sysu.edu.cn).}



\thanks{Yulan Guo is with the School of
Electronics and Communication Engineering, Sun Yat-sen University,
Shenzhen Campus, Shenzhen 510006, China (E-mail: guoyulan@sysu.edu.cn).}
\thanks{Bin Hu is with the School of Medical Technology, Beijing Institute of Technology, Beijing 100081, China (E-mail: bh@bit.edu.cn). }
\thanks{Tat-Seng Chua is with the School of Computing, National University of Singapore, Singapore 119077 (E-mail: dcscts@nus.edu.sg). }
\thanks{*Corresponding author: Jisheng Dang, Bimei Wang, and Bin Hu.}
}

\maketitle
% --- ABSTRACT ---
\begin{abstract}
    Vision-Language Models (VLMs) have demonstrated impressive capabilities in multimodal tasks, yet they persistently suffer from visual hallucinations and language shortcuts, where the model's reasoning disconnects from visual perception. Recent approaches, such as Vision-SR1, have attempted to mitigate this via a ``self-rewarding'' paradigm using reasoning decomposition. While pioneering, these methods are constrained by a static ``See-Think'' structure, coarse-grained binary rewards, and a passive validation mechanism that fails to learn from errors. In this paper, we propose a unified Self-Evolving Framework that fundamentally addresses these limitations. Our approach introduces three key contributions: (1) \textbf{Hierarchical Self-Rewarding with Early Exit (HSR-EE)}, a dynamic inference graph that adaptively adjusts reasoning depth based on problem complexity; (2) \textbf{Iterative Perception Refinement (IPR)}, a closed-loop data engine that actively transforms validation failures into high-quality preference pairs; and (3) \textbf{Self-Rewarding Direct Preference Optimization (SR-DPO)}, which leverages the VLM itself as a fine-grained judge to optimize visual grounding. Extensive experiments on MMMU, MM-Vet, and POPE benchmarks demonstrate that our framework not only outperforms strong baselines but also reduces language shortcut dependency by over 40\%, establishing a new state-of-the-art in self-improving multimodal systems.
    Code is available at \url{https://github.com/tangfenga/SR2SE}.
\end{abstract}

% --- FIGURE 8: MOTIVATION (TOP OF SECOND PAGE, TWO COLUMNS) ---
\begin{figure*}[!t]
    \centering
    \vspace{-0.6em}
    \includegraphics[width=0.95\textwidth,keepaspectratio]{figures/motivation.pdf}
    \vspace{-1.0em}
    \caption{Motivation of the proposed self-evolving framework.}
    \label{fig:motivation}
\end{figure*}

% --- KEYWORDS ---ref_vision_sr1
\begin{IEEEkeywords}
    Vision-language models, reinforcement learning, direct preference optimization, hierarchical reasoning, self-correction, visual hallucination.
\end{IEEEkeywords}

% -------------------------------------------------------------------
% --- SECTION 1: INTRODUCTION ---
% -------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}

\IEEEPARstart{T}{he} rapid evolution of Large Vision-Language Models (VLMs) has unlocked impressive capabilities in multimodal reasoning~\cite{ref_gpt4v, ref_llava}. However, a critical misalignment persists: the ``modality gap,'' where the powerful language decoder overpowers the visual encoder. This leads to severe reliability issues such as \textit{visual hallucination} and \textit{language shortcuts}~\cite{ref_hallucination_survey}. In high-stakes domains like medical diagnosis or autonomous driving, a model that answers correctly based on linguistic priors rather than visual evidence is not just flawed, it is dangerous.

Traditional alignment methods like supervised sine-tuning (SFT) or reinforcement learning from human feedback (RLHF)
struggle to mitigate this because they rely on ``outcome-based'' supervision.
They reward the final answer, failing to penalize the flawed intermediate
reasoning that often leads to the correct answer by chance. To address this,
recent self-rewarding paradigms, such as Vision-SR1~\cite{ref_vision_sr1},
decompose reasoning into perception and thought, validating the perception's
sufficiency. While Reasoning Decomposition has shown promise, this
``First-Generation Self-Rewarding'' paradigm faces three fundamental
bottlenecks inherent to its static nature:

The efficiency-depth paradox. Existing pipelines enforce a uniform computational budget regardless of task difficulty. This rigidity leads to a dual failure mode: wasting resources on trivial queries while lacking sufficient reasoning depth for complex logical puzzles,the model to hallucinate details it effectively glanced over rather than reasoned through.
The gradient vanishing problem in binary reward mechanisms is inherently precipitated by the reliance on discrete $\{0, 1\}$ signals, which lack the requisite granularity to characterize complex output distributions and guide effective optimization.Relying on coarse binary signals ($0$ or $1$) fails to capture the degree of hallucination. The model receives the same penalty for a minor color error as for a complete fabrication, limiting fine-grained alignment.

Current methodologies are fundamentally constrained by an open-loop architecture that overlooks the informative richness of suboptimal inference outcomes. The conventional practice of discarding error trajectories as redundant data ignores the critical feedback signals contained within these failures, effectively stifling the model’s capacity for iterative refinement and learning from its own performance history.

Addressing the structural bottlenecks of first-generation self-rewarding VLMs reveals several fundamental limitations that impede their efficacy as autonomous learners. Conventional frameworks typically employ a rigid perception-reasoning architecture that enforces a uniform computational budget across all queries, failing to adapt reasoning depth to the inherent complexity of individual instances. This rigidity is compounded by the reliance on coarse-grained, binary reward signals that focus solely on final outcomes, which lacks the necessary granularity to distinguish between minor perceptual discrepancies and catastrophic visual hallucinations. Furthermore, existing validation mechanisms are primarily passive and unidirectional; once inference is concluded, valuable failure trajectories are discarded as waste rather than being recycled into a constructive data engine for continuous policy optimization.

These observations motivate a more principled self-evolving paradigm that treats
inference, correction, and optimization as a coupled closed-loop system rather
than disjoint components.
The proposed unified self-evolving framework facilitates a fundamental transition for Vision-Language Models from static solvers into active learners. This architecture is established upon a closed-loop system encompassing adaptive inference scaling, autonomous perceptual refinement, and robust policy optimization, which collectively enable the model to identify and internalize its own reasoning failures as constructive learning signals.

The framework overcomes computational rigidity via hierarchical self-rewarding with early exit, acknowledging that perceptual hallucinations often emerge from insufficient reasoning depth on complex visual stimuli. By incorporating an adaptive inference termination mechanism, the system dynamically modulates its computational budget, preserving efficiency for elementary perception while extending reasoning chains for multifaceted queries to prevent superficial or ungrounded responses.

Complementing this dynamic inference is a closed learning loop facilitated by iterative perception refinement, which treats failed trajectories as informative precursors for contrastive learning rather than discarding them as extraneous waste. This module mandates a rigorous re-examination of the visual context to capture previously neglected details, enabling the autonomous synthesis of high-fidelity preference pairs that contrast corrected perceptions against initial errors.
These synthesized refinements are subsequently internalized through self-rewarding direct preference optimization, which leverages the on-policy hard negatives identified during refinement to provide a robust objective for visual grounding. In contrast to traditional reinforcement learning paradigms that suffer from gradient vanishing due to binary signals, this approach utilizes fine-grained preference gradients to strictly align model outputs with grounded evidence, thereby enhancing optimization stability and mitigating the risk of reward exploitation.

\noindent In summary, our contributions can be outlined as follows.
\begin{itemize}
    \item We introduce HSR-EE, a dynamic reasoning architecture that adaptively adjusts inference depth according to task complexity, enabling a balanced trade-off between efficiency and reasoning completeness.
    \item We develop IPR, an active self-correction mechanism that transforms unsuccessful model trajectories into informative preference signals, forming a closed-loop data engine for continuous improvement.
    \item We propose SR-DPO, a fine-grained optimization objective that mitigates reward hacking in binary reinforcement learning and enhances the precision of visual grounding.
    \item Extensive experiments on MMMU, MM-Vet, and POPE demonstrate that the proposed framework reduces language shortcuts and establishes a new state-of-the-art in self-improving multimodal systems.
\end{itemize}



% -------------------------------------------------------------------
% --- SECTION 2: RELATED WORK ---
% -------------------------------------------------------------------
% --- FIGURE 1: TEASER (TOP OF THIRD PAGE) ---
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/overview.pdf}
    \caption{Overview of the proposed Self-Evolving Framework. Our approach consists of three key components: (1) Hierarchical Self-Rewarding with Early Exit (HSR-EE) for adaptive reasoning depth, (2) Iterative Perception Refinement (IPR) that transforms validation failures into preference pairs, and (3) Self-Rewarding Direct Preference Optimization (SR-DPO) for fine-grained visual grounding alignment.}\label{fig:framework}
\end{figure*}

\section{Related Work}

\subsection{From Instruction Following to System 2 Reasoning}
Early VLMs, such as LLaVA-1.5~\cite{ref_llava} and GPT-4V~\cite{ref_gpt4v},
primarily relied on System~1 style intuitive pattern matching, aligning visual
encoders with LLMs to achieve immediate instruction following. However, this
paradigm often struggles with complex tasks requiring multi-step logical
deduction, leading to severe hallucinations~\cite{ref_hallucination_survey}.
To bridge the modality–logic gap, research in 2024--2025 shifted toward
developing System~2 style reasoning capabilities. DeepSeek-R1~\cite{ref_deepseek_r1}
demonstrated in the text domain that reinforcement learning can incentivize
Chain-of-Thought (CoT) reasoning. In the multimodal domain, LLaVA-o1~\cite{ref_llava_o1}
introduced a structured reasoning phase (Summary–Caption–Reasoning–Conclusion),
significantly improving performance on logic-intensive tasks. Similarly,
Insight-V~\cite{ref_insight_v} designed a multi-agent architecture that
separates reasoning generation from result summarization to support extended
reasoning chains.

While Vision-SR1~\cite{ref_vision_sr1} pioneered the idea of reasoning
decomposition, it can be regarded as a proto-System~2 approach: it separates
perception from reasoning but adopts a static pipeline. Such rigidity limits
its ability to capture the dynamic nature of System~2 reasoning. Our work
extends this paradigm by introducing a hierarchical reasoning graph that lifts
the static constraint. Unlike Vision-SR1, the proposed framework enables the
model to adaptively engage in deeper reasoning only when necessary, effectively
implementing a System~2 style mechanism that mirrors human cognitive resource
allocation.


\subsection{Self-Evolving and Intrinsic Correction Frameworks}
Self-evolving AI has emerged as an effective direction for reducing reliance on
high-quality annotated data. VisPlay~\cite{ref_visplay} introduced a
co-evolutionary framework based on unlabeled images, where a Questioner and a
Reasoner jointly construct training curricula, enabling exploration-driven
capability expansion. Sherlock~\cite{ref_sherlock} investigated intrinsic
self-correction, training models to revise their own reasoning errors without
external feedback using trajectory-level objective functions. In addition,
EvoLMM~\cite{ref_evolmm} and Agent0~\cite{ref_agent0} demonstrated that models
can autonomously generate learning signals for tool use and general tasks.

In contrast to VisPlay's exploratory task generation, our Iterative Perception
Refinement (IPR) module emphasizes error-driven grounding. Empirically, visual
hallucinations often arise when the model attends to images superficially
rather than performing detailed scrutiny. The IPR mechanism does not generate
new tasks; instead, it identifies perceptual failures on existing tasks and
compels the model to re-examine the image through refinement prompts. This is
conceptually related to Vision-SR1~\cite{ref_vision_sr1}, but our formulation
establishes a fully closed-loop system. Unlike Sherlock~\cite{ref_sherlock} or
REVERSE~\cite{ref_reverse}, which primarily leverage correction for
inference-time refinement or supervised data construction, we convert corrected
trajectories into preference pairs for direct policy optimization. This enables
the model to internalize the correction logic, effectively transforming
temporary inference adjustments into persistent policy improvements.

\subsection{Inference-Time Compute and Test-Time Scaling}
As model sizes and application demands increase, managing inference-time
compute has become a central research focus. Early studies on test-time scaling
in text-only LLMs demonstrated that allocating additional decoding budget at
inference can be more effective than merely scaling model parameters, leading
to the formulation of inference-time scaling laws~\cite{ref_inference_scaling}.
Approaches such as Chain-of-Thought (CoT) prompting and self-consistency
encourage models to produce longer, structured rationales, while Tree-of-Thoughts
(ToT) and planning-based decoders explore multiple reasoning branches before
committing to a final answer. However, these techniques typically assume a fixed
or manually tuned sampling budget (e.g., number of samples or reasoning depth)
that remains uniform across queries.

In the multimodal setting, inference is further complicated by the computational
cost of visual encoders and the requirement to align textual reasoning with
image evidence. Existing System~2 style VLMs~\cite{ref_llava_o1,ref_insight_v}
extend CoT or ToT methodologies to perception-grounded tasks, yet they continue
to execute pre-defined reasoning scripts (e.g., Summary--Caption--Reasoning--Conclusion)
regardless of task difficulty. In parallel, the adaptive computation community
has explored early-exit mechanisms at the architectural level. For example,
AD-EE~\cite{ref_ad_ee} employs causal reasoning to select Transformer exit layers
to satisfy real-time constraints, while FREE~\cite{ref_free} optimizes early-exit
classifiers through adversarial training. Mixture-of-Depths (MoD)~\cite{ref_mod}
reduces compute by dynamically skipping layers or tokens.

Our HSR-EE module connects these research directions from a semantics-oriented
perspective. Instead of truncating physical layers based solely on entropy or
confidence, HSR-EE explicitly models the reasoning trajectory and attaches a
confidence head $g_{\phi}$ to natural-language sub-goals. This enables
instance-wise test-time scaling over reasoning depth: simple perception queries
terminate after one or two steps, whereas complex multi-hop problems trigger
deeper reasoning chains. In contrast to standard CoT/ToT methods that apply a
fixed reasoning template, HSR-EE learns when additional visual attend--reason
cycles genuinely improve accuracy under a compute budget, resulting in inference-time
scaling that is adaptive, grounded, and computation-aware.


\subsection{Preference Optimization and Hallucination Mitigation}
Hallucination mitigation in VLMs has been studied from multiple perspectives.
Outcome-based RLHF pipelines align models with human preferences by training a
reward model and subsequently applying PPO-style optimization. Although
effective, these methods are often sample-inefficient and sensitive to reward
misspecification and value network instability. Decoding-based strategies,
including constrained decoding, nucleus/top-$k$ sampling, and calibrated
temperature scaling, can reduce over-confident generations but do not correct
the model's internal representations; instead, they primarily adjust the
sampling distribution. Another direction incorporates external knowledge bases
or retrievers to provide evidence during inference, but such approaches
increase system complexity and introduce dependency on external resources.

Within this landscape, preference optimization has emerged as a stable and
computationally efficient alternative to full reinforcement learning.
While PPO was widely adopted in early RL-based frameworks, its training
instability motivated the development of Direct Preference Optimization
(DPO)~\cite{ref_dpo}. More recent work, including SimPO~\cite{ref_simpo} and
ORPO~\cite{ref_orpo}, further introduced reference-free schemes that improve
efficiency by directly optimizing log-probability margins. In the multimodal
setting, RLAIF-V~\cite{ref_rlaif_v} demonstrated the effectiveness of using
stronger models to construct fine-grained preference data, while
Silkie~\cite{ref_silkie} and V-DPO~\cite{ref_vdpo} studied distillation-based
preference learning.

However, most multimodal preference optimization methods rely on external
supervision or pre-collected datasets to define preference pairs. This reliance
introduces distribution mismatch, as negative samples may not correspond to the
student model's current failure modes, and it limits scalability when new
domains or tasks arise. In contrast, our SR-DPO constructs preference pairs
directly from model-generated trajectories via the IPR module. By contrasting
pre-correction and post-correction outputs on the same image-question pair, the
preference optimization objective remains strictly on-policy. This yields
fine-grained gradients that target the specific visual details the model tends
to overlook, rather than optimizing against generic or teacher-defined
failures. From the perspective of hallucination mitigation, the proposed
framework situates itself at the intersection of intrinsic self-correction and
preference-based alignment, providing a scalable approach for reducing
hallucinations without reliance on extensive human annotation or external
oracles.

% --- FIGURE 7: OVERALL CAPABILITY RADAR CHART (ENLARGED, SECOND PAGE) ---
\begin{figure}[!t]
    \centering
    \includegraphics[width=1.05\columnwidth]{figures/blance.pdf}
    \caption{Radar chart comparing overall capabilities of different models across four dimensions: reasoning (MMMU), integrated multimodal skills (MM-Vet), hallucination suppression (POPE F1) and language shortcut rate (LSR, inverted scale). Our method achieves a more balanced and expanded profile, indicating consistent gains rather than trading off one capability for another.}
    \label{fig:radar_overall}
\end{figure}

% -------------------------------------------------------------------
% --- SECTION 3: METHODOLOGY ---
% -------------------------------------------------------------------
\section{Methodology}\label{sec:methodology}

We present a unified framework to improve the visual grounding and reasoning capabilities of VLMs. The central goal is transitioning from a static, supervised learning regime to a dynamic, self-evolving paradigm that uses intrinsic correction signals.

\subsection{Problem Formulation and Baseline}
Let $\mathcal{V}$ be a VLM parameterized by $\theta$. Given an image $I$ and a query $Q$, the model generates a response $S$. The Vision-SR1 baseline decomposes this response into a visual perception $c$, a reasoning component $t$, and a final answer $a$. The objective maximizes the expected reward
\begin{equation}
    J(\theta) = \mathbb{E}_{(I,Q) \sim \mathcal{D}} [r(c, t, a)] ,
\end{equation}
capturing the contribution of each component within the self-rewarding pipeline. Vision-SR1 uses a binary consistency reward, where $r = 1$ if $\mathcal{V}(c, Q) \rightarrow a_{gt}$ and $0$ otherwise. Such binary signals lack the granularity required for fine-grained learning.

\subsubsection{Notation}
We parameterize the vision-language model as $\mathcal{V}(\theta)$. Prompting extracts the sub-policies $\pi_{perc}$, $\pi_{reason}$, and $\pi_{text}$ from $\mathcal{V}$; specifically, $\pi_{perc}(I, Q)$ yields the perception $c$. During training for IPR data construction, the ground truth answer $a_{gt}$ is drawn from the labeled dataset $\mathcal{D}$. At inference time, $a_{gt}$ is unavailable, requiring the model to rely entirely on its learned policy. To evaluate the outputs, we define $r_{visual}$ as a binary score indicating whether the perception $c$ is grounded in the image $I$, and $r_{ans}$ as the correctness score $\mathbb{I}(a = a_{gt})$.

\subsection{Hierarchical Self-Rewarding with Early Exit (HSR-EE)}\label{sec:hsr_ee}

% --- FIGURE 5: HSR-EE MECHANISM ILLUSTRATION ---
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/visualization.pdf}
    \caption{(Left) Case study comparing the previous self-rewarding pipeline with our method, demonstrating reduced hallucinations and more grounded reasoning. (Right) Effect of IPR iterations and performance improvements across multiple vision-language benchmarks, highlighting enhanced accuracy and robustness.}
    \label{fig:visualization}
\end{figure*}

Standard CoT approaches assume a fixed computational budget for every token. Visual reasoning is inherently hierarchical: some questions require only surface-level perception, such as asking ``Is there a cat?'', while others require multi-hop reasoning, such as determining ``Is the cat acting aggressively towards the dog?''.

The inference process can be modeled as a dynamic graph. At each step $k$, the model maintains a history state $H_k$ and the policy $\pi_\theta$ generates a sub-goal or reasoning step $z_k$:
\begin{equation}
    H_k = \{z_1, z_2, \dots, z_k\}, \qquad z_k \in \mathcal{Z},
\end{equation}
where $z_k$ is a natural language sentence representing a sub-goal like ``Check the color of the object''.

From a decision-theoretic perspective, the choice of whether to continue reasoning at step $k$ or to stop and answer can be cast as an optimal stopping problem under a finite compute budget. Let $T$ denote the random stopping time at which the model exits, and let $C_k$ denote the cumulative compute cost up to step $k$. Given a total budget $C$, the goal is to maximize the expected accuracy $A$ subject to a budget constraint:
\begin{equation}
    \max_{\pi_\theta, g_\phi} \; \mathbb{E}[A(T)] \quad \text{s.t.} \quad \mathbb{E}[C_T] \le C .
\end{equation}
At each step $k$, the model observes $H_k$ and must decide between continuing ($T > k$) and stopping ($T = k$). The probability that the current reasoning state already yields a correct answer is denoted as:
\begin{equation}
    p_k = \Pr(a = a_{gt} \mid H_k) .
\end{equation}
Under a myopic approximation of optimal stopping, the model should stop when the expected utility of stopping exceeds that of taking one more step:
\begin{equation}
    p_k - \lambda C_k \ge \mathbb{E}[p_{k+1} - \lambda C_{k+1} \mid H_k] ,
\end{equation}
where $\lambda$ is the Lagrange multiplier associated with the compute budget. The confidence head $g_\phi$ learns a surrogate of $p_k$ from data and implements this stopping rule via a threshold on its output.

\subsubsection{Dynamic Loop}
The hierarchical reasoning loop can be written as
\begin{align}
    c   & = \pi_{perc}(I, Q) ,           \\
    z_1 & = \pi_{reason}(c, Q) ,        \\
        & \vdots                        \\
    z_k & = \pi_{reason}(c, Q, H_{k-1}) ,
\end{align}
where $H_{k-1} = \{z_1, \dots, z_{k-1}\}$. This loop is implemented as an
autoregressive generation process where the model decides at each step whether
to continue reasoning or output the final answer.

\subsubsection{Early Exit Mechanism}
To enable adaptive computation, we introduce a confidence head $g_\phi$ trained
to predict the sufficiency of the current history $H_k$. The head $g_\phi$ is a
lightweight Multi-Layer Perceptron (MLP) that takes the hidden state of the
last token of $z_k$ as input and projects it to a scalar score.
\begin{equation}
    \text{Score}_k = \sigma(g_\phi(H_k)).
\end{equation}
If $\text{Score}_k > \tau$ (a learned threshold), the model terminates the reasoning loop and generates the final answer $a$. This allows the model to ``exit early'' for simple queries.

The inference procedure is summarized in Algorithm \ref{alg:hsr_ee}.

\begin{algorithm}
    \caption{HSR-EE Inference}
    \label{alg:hsr_ee}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Image $I$, Query $Q$
        \STATE \textbf{Output:} Answer $a$
        \STATE $c \leftarrow \pi_{perc}(I, Q)$ \COMMENT{Perception via prompting}
        \STATE $H \leftarrow []$ \COMMENT{Initialize history}
        \STATE $k \leftarrow 0$
        \WHILE{True}
        \STATE $z_k \leftarrow \pi_{reason}(c, Q, H)$ \COMMENT{Generate next reasoning step}
        \STATE $H.\text{append}(z_k)$
        \STATE $k \leftarrow k + 1$
        \STATE $\text{Score}_k \leftarrow \sigma(g_\phi(H))$ \COMMENT{Compute confidence}
        \IF{$\text{Score}_k > \tau$}
        \STATE \textbf{break}
        \ENDIF
        \ENDWHILE
        \STATE $a \leftarrow \text{GenerateAnswer}(H)$
        \RETURN $a$
    \end{algorithmic}
\end{algorithm}

To train the confidence head $g_{\phi}$, we treat the decision to exit or
continue as a binary classification task. We construct pseudo-labels $y_{gate}$
during training: if the current perception $c$ leads to the correct answer
$a_{gt}$, then $y_{gate}=1$ (Exit); otherwise $y_{gate}=0$ (Continue). The loss
function for the gating module is defined as:
\begin{equation}
    \mathcal{L}_{gate} = - \mathbb{E}_k \big[ y_{gate} \log p_k + (1-y_{gate}) \log (1-p_k) \big],
\end{equation}
where $p_k = \sigma(g_{\phi}(H_k))$. This cross-entropy objective encourages
$p_k$ to approximate the true success probability $\Pr(a=a_{gt}\mid H_k)$. As
long as the training data contains both successful and failed trajectories for
different $H_k$, the global minima of $\mathcal{L}_{gate}$ correspond to
non-degenerate $p_k \in (0,1)$, preventing the gate from collapsing to
always-stop or always-continue policies.

The reward for this module includes a penalty for length, encouraging
efficiency:
\begin{equation}
    R_{HSR} = r_{visual} + r_{ans} - \lambda \cdot k.
\end{equation}
where $r_{ans} = \mathbb{I}(a = a_{gt})$ is the answer correctness reward, and $r_{visual}$ is a heuristic reward measuring the overlap between the generated perception $c$ and the visual ground truth (if available) or a consistency score.

% --- FIGURE 4: EFFICIENCY ANALYSIS ---
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/figure4.png}
    \caption{Efficiency analysis: HSR-EE balances accuracy and token cost. Early Exit reduces average reasoning steps while maintaining or improving MM-Vet performance versus fixed-depth baselines.}\label{fig:efficiency}
\end{figure*}

\subsubsection{Theoretical Analysis of Early Exit}
The decision process of HSR-EE can be viewed as a two-action Markov Decision
Process (MDP), where each state $H_k$ summarizes the reasoning history up to
step $k$, and the available actions are $a_k \in \{\mathrm{Continue},\mathrm{Exit}\}$.
Transitioning with $a_k=\mathrm{Continue}$ yields the next state $H_{k+1}$ with
transition kernel $P(H_{k+1}\mid H_k)$ induced by the autoregressive policy
$\pi_\theta$, while $a_k=\mathrm{Exit}$ terminates the episode and triggers a
terminal reward. A natural reward function consistent with $R_{HSR}$ is
\begin{equation}
    r(H_k, a_k) =
    \begin{cases}
        r_{visual} + r_{ans} - \lambda k, & a_k = \mathrm{Exit}, \\[2pt]
        -\lambda,                          & a_k = \mathrm{Continue},
    \end{cases}
\end{equation}
where the per-step cost $-\lambda$ reflects compute consumption and the
terminal reward encodes answer correctness and visual grounding. Let
\begin{equation}
    V^*(H_k) = \max_{\pi} \, \mathbb{E}_\pi\big[ R_{HSR} \mid H_k \big]
\end{equation}
denote the optimal value function. The Bellman optimality equation is
\begin{equation}
    V^*(H_k) = \max \big\{ Q^*(H_k, \mathrm{Exit}),\; Q^*(H_k, \mathrm{Continue}) \big\},
\end{equation}
with
\begin{align}
    Q^*(H_k, \mathrm{Exit})      & = r_{visual} + r_{ans} - \lambda k,                                      \\
    Q^*(H_k, \mathrm{Continue}) & = -\lambda + \mathbb{E}[ V^*(H_{k+1}) \mid H_k, a_k=\mathrm{Continue} ].
\end{align}
The optimal stopping rule is therefore to choose $\mathrm{Exit}$ iff
\begin{equation}
    Q^*(H_k, \mathrm{Exit}) \ge Q^*(H_k, \mathrm{Continue}).
\end{equation}
In practice $V^*$ is unknown and expensive to approximate. Our myopic
approximation replaces $V^*(H_k)$ by the immediate terminal utility
\begin{equation}
    U(H_k) = p_k \cdot (1-\lambda k) + (1-p_k) \cdot (-\lambda k),
\end{equation}
where $p_k = \Pr(a=a_{gt}\mid H_k)$ is the success probability. Under this
approximation, exiting at step $k$ is preferred over continuing when
\begin{equation}
    p_k - \lambda k \ge \mathbb{E}[ p_{k+1} - \lambda (k+1) \mid H_k ],
\end{equation}
which recovers the heuristic rule in (3). If we further assume that
$p_{k+1}-p_k$ is non-increasing in $k$ (diminishing returns of additional
reasoning) and that the per-step cost $\lambda$ is constant, it can be shown
that the resulting stopping objective is discrete-concave and admits a
simple characterization via marginal gains. Below we provide an explicit regret
bound that formalizes when a greedy early-exit threshold is near-optimal.

\noindent\textit{Connection to the implementation.}
Because the confidence head is an MLP followed by a sigmoid, we can interpret
its pre-sigmoid logit as any scalar surrogate of the stopping advantage (e.g.,
$\lambda-\widehat{\Delta}_k$). Therefore, the practical gate $\text{Score}_k>\tau$
implements a marginal-gain threshold policy up to a monotone reparameterization
of the output.

\noindent\textbf{Proposition 1 (Regret bound for greedy early exit).}
Consider the myopic stopping objective
\begin{equation}
    J(k) \triangleq \mathbb{E}[p_k] - \lambda k, \qquad k\in\{1,\dots,K\},
\end{equation}
and define the (static) optimal stopping index $k^\star\in\arg\max_k J(k)$. Let
the expected one-step marginal gain be
\begin{equation}
    \Delta_k \triangleq \mathbb{E}[p_{k+1}-p_k], \qquad k\in\{1,\dots,K-1\}.
\end{equation}
Assume (i) \emph{diminishing returns}: $\Delta_{k+1} \le \Delta_k$ for all $k$;
and (ii) the gate provides a uniformly accurate estimate of marginal gains,
$|\widehat{\Delta}_k-\Delta_k|\le \varepsilon$.
Let the greedy stopping rule be the first index
\begin{equation}
    \hat{k} \triangleq \min\{k: \widehat{\Delta}_k \le \lambda\},
\end{equation}
with $\hat{k}=K$ if the set is empty.
Then the myopic regret
\begin{equation}
    \mathcal{R} \triangleq J(k^\star) - J(\hat{k})
\end{equation}
is bounded by
\begin{equation}
    \mathcal{R} \le \varepsilon \cdot W_{\varepsilon}, \qquad
    W_{\varepsilon} \triangleq \big|\{k:\, |\Delta_k-\lambda|\le \varepsilon\}\big|,
\end{equation}
and in particular $\mathcal{R}\le \varepsilon$ when the crossing of
$\Delta_k$ around $\lambda$ is sharp (i.e., $W_{\varepsilon}=1$).

\noindent\textit{Proof (sketch).}
Since $J(k+1)-J(k)=\Delta_k-\lambda$ and $\{\Delta_k\}$ is non-increasing, $J(k)$
is discrete-concave and attains its maximum at (or within) the index region
where $\Delta_k$ crosses $\lambda$. The rule $\hat{k}$ can differ from $k^\star$
only inside the near-indifference band $|\Delta_k-\lambda|\le \varepsilon$,
because outside this band the sign of $\Delta_k-\lambda$ is robust to an
$\varepsilon$ estimation error and the greedy decision matches the oracle
threshold. Summing the worst-case per-step utility gap (at most $\varepsilon$)
over the steps in this band yields $\mathcal{R}\le \varepsilon W_{\varepsilon}$.
$\hfill\Box$

\noindent\textit{Remark.}
The diminishing-returns assumption is natural when each additional reasoning
step adds an evidence unit to a growing set and the success probability
$p_k$ is induced by a monotone submodular utility over that evidence, in which
case the expected marginal gains $\Delta_k$ decrease with $k$.

From a learning perspective, our confidence head $g_\phi$ approximates the
mapping $H_k \mapsto p_k$. If $g_\phi$ is calibrated, the induced policy that
exits when $\sigma(g_\phi(H_k)) > \tau$ realizes a parametric approximation of
the optimal stopping rule. Practically, the gate can be trained to approximate
either $p_k$ directly or the continue-vs-exit advantage (a monotone
transform of $\Delta_k-\lambda$); in both cases, a thresholding rule implements
a one-step lookahead policy whose regret is controlled when marginal gains
exhibit diminishing returns and the gate is sufficiently accurate.

\subsection{Overall Training Pipeline}

Our full framework alternates between supervised fine-tuning, IPR-based data
generation, and SR-DPO optimization. Algorithm~\ref{alg:training} summarizes
the overall pipeline.

\begin{algorithm}
    \caption{Overall Training Pipeline}
    \label{alg:training}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Labeled dataset $\mathcal{D}$, backbone VLM $\mathcal{V}$
        \STATE \textbf{Output:} Aligned model $\mathcal{V}_{\text{aligned}}$
        \STATE \textbf{Stage 1: SFT Initialization}
        \STATE Fine-tune $\mathcal{V}$ on instruction-following data to obtain $\pi_{ref}$
        \STATE \textbf{Stage 2: IPR-based Preference Construction}
        \STATE Initialize empty preference buffer $\mathcal{D}_{pref}$
        \FOR{each $(I, Q, a_{gt}) \in \mathcal{D}$}
            \STATE Run HSR-EE inference to obtain $(c, t, a)$ and reward $R_{HSR}$
            \STATE Use Algorithm~\ref{alg:ipr} to generate $(y_w, y_l)$ if possible
            \IF{a pair is returned}
                \STATE Add $(x=(I,Q), y_w, y_l)$ to $\mathcal{D}_{pref}$
            \ENDIF
            \STATE Update gate parameters $g_\phi$ using $R_{HSR}$ and $\mathcal{L}_{gate}$
        \ENDFOR
        \STATE \textbf{Stage 3: SR-DPO Optimization}
        \FOR{multiple epochs}
            \STATE Sample mini-batches from $\mathcal{D}_{pref}$
            \STATE Update policy parameters $\theta$ by minimizing $\mathcal{L}_{SR\text{-}DPO}$
        \ENDFOR
        \STATE \textbf{return} $\mathcal{V}_{\text{aligned}}$ with updated $\theta$ and $g_\phi$
    \end{algorithmic}
\end{algorithm}

% --- FIGURE 2: IPR PROCESS ---
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.95\columnwidth]{figures/figure2.png}
%     \caption{IPR acts as a data engine. By prompting the model to correct its own insufficient descriptions, we autonomously generate high-quality preference data.}\label{fig:ipr}
% \end{figure}

\subsection{Iterative Perception Refinement (IPR)}\label{sec:ipr}

We explicitly distinguish between the training and inference phases. IPR
functions as an \textbf{offline data generation engine} used to construct the
preference dataset $\mathcal{D}_{pref}$. During this phase, we leverage access
to ground truth labels ($a_{gt}$) to validate perceptions. This allows us to
synthesize high-quality ``Winner-Loser'' pairs from the model's own failure
trajectories, which are subsequently used to train the policy via SR-DPO for
robust inference where $a_{gt}$ is unavailable. While this reliance on $a_{gt}$
makes the current implementation ``semi-self-evolving,'' it ensures the quality
of the training signal. Future work can replace $a_{gt}$ with model-based
consistency checks for fully autonomous evolution.

The core innovation of our framework is the ability to learn from failure. In
standard RL, a trajectory with 0 reward is simply a weak negative signal. In
our framework, a 0-reward trajectory is a seed for generating a strong training
signal.

\subsubsection{Detection}
We first generate an initial perception $c_0$. We perform the self-validation
check:
\begin{equation}
    \text{Valid}(c_0) = \mathbb{I}(\pi_{text}(Q, c_0) == a_{gt})
\end{equation}
If $\text{Valid}(c_0) = 0$, it implies $c_0$ missed critical visual details required for the answer.

\subsubsection{Refinement}
Upon failure, we trigger the IPR module. We inject a \textit{Refinement Prompt}
back into the model. The prompt template is designed to guide the model to
re-examine the image, as shown in Figure \ref{fig:refinement_prompt}.

% --- FIGURE 3: REFINEMENT PROMPT AS BOXED FIGURE ---
\begin{figure}[!t]
    \centering
    \begin{tcolorbox}[width=0.95\columnwidth,colback=white,colframe=black!40,boxrule=0.6pt]
        {\bfseries Refinement Prompt Template}\\[2pt]
        {\itshape ``The description '$c_0$' was insufficient to answer the question correctly. Please re-examine the image. Focus on spatial relationships and small details that were missed. Provide a refined description.''}
    \end{tcolorbox}
    \caption{The refinement prompt used in the IPR module to guide the model to re-examine the image upon failure. The boxed layout turns the textual template into a visual element, improving readability and occupying sufficient vertical space.}
    \label{fig:refinement_prompt}
\end{figure}

The model generates $c_{refined}$. We repeat the validation check. If
$\text{Valid}(c_{refined}) = 1$, we deem this a successful correction. To
prevent hallucination accumulation, we limit the refinement process to a
maximum of 2 iterations. If the model fails to correct itself after 2 attempts,
the sample is discarded to avoid introducing noise into the preference dataset.

\subsubsection{Pair Construction}
We now have a preference pair suitable for contrastive learning:
	extbf{Chosen ($y_w$):} $c_{refined}$ (\textbf{The high-fidelity perception}); \textbf{Rejected ($y_l$):} $c_0$ (\textbf{The hallucinated or incomplete perception}).
This dataset $\mathcal{D}_{pref}$ is dynamically constructed during training, ensuring the distribution matches the model's current weaknesses.

\begin{algorithm}
    \caption{IPR Data Generation Process}
    \label{alg:ipr}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Training example $(I, Q, a_{gt})$
        \STATE \textbf{Output:} Optional preference pair $(y_w, y_l)$
        \STATE $c_0 \leftarrow \pi_{perc}(I, Q)$ \COMMENT{Initial perception}
        \STATE $\text{valid} \leftarrow \mathbb{I}(\pi_{text}(Q, c_0) = a_{gt})$
        \IF{$\text{valid} = 1$}
            \STATE \textbf{return} \textit{None} \COMMENT{No failure, no pair}
        \ENDIF
        \STATE $c_{refined} \leftarrow c_0$
        \FOR{$i = 1$ to $N_{\max}$}
            \STATE \COMMENT{$N_{\max}=2$ in our experiments}
            \STATE $c_{refined} \leftarrow \mathrm{Refine}(I, Q, c_{refined})$ \COMMENT{Refine perception using the refinement prompt}
            \STATE $\text{valid} \leftarrow \mathbb{I}(\pi_{text}(Q, c_{refined}) = a_{gt})$
            \IF{$\text{valid} = 1$}
                \STATE $y_w \leftarrow c_{refined}$, $y_l \leftarrow c_0$
                \STATE \textbf{return} $(y_w, y_l)$
            \ENDIF
        \ENDFOR
        \STATE \textbf{return} \textit{None} \COMMENT{Refinement failed, discard sample}
    \end{algorithmic}
\end{algorithm}

\subsection{Self-Rewarding Direct Preference Optimization (SR-DPO)}
\label{sec:sr_dpo}

With the preference dataset $\mathcal{D}_{pref}$ generated by IPR, we move
beyond simple scalar rewards. We employ Direct Preference Optimization (DPO) to
align the policy. The SR-DPO module integrates the preference pairs generated
by IPR to optimize the full VLM policy $\pi_\theta$, while the HSR-EE reward
$R_{HSR}$ is used separately to train the gating mechanism $g_\phi$.

The standard DPO objective is derived from the KL-constrained reward
maximization problem. In our context, we minimize the negative log-likelihood
of the preferred perception:
\begin{multline}
    \mathcal{L}_{SR\text{-}DPO} = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \Bigg[ \log \sigma \Bigg( \\
        \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \Bigg) \Bigg]
\end{multline}
where $x = (I, Q)$ is the visual context and query, and $\pi_{ref}$ is the reference model (the SFT checkpoint).

To better understand why SR-DPO is particularly effective for hallucination
mitigation, we analyze its gradient. Denote
\begin{equation}
    f_\theta(x, y_w, y_l) = \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}.
\end{equation}
Then the SR-DPO loss can be written as
\begin{equation}
    \mathcal{L}_{SR\text{-}DPO} = - \mathbb{E}_{(x,y_w,y_l)} [ \log \sigma(f_\theta(x,y_w,y_l)) ].
\end{equation}
Taking the gradient w.r.t.~$\theta$ yields
\begin{equation}
    \nabla_\theta \mathcal{L}_{SR\text{-}DPO} = - \mathbb{E} \big[ (1-\sigma(f_\theta)) \, \nabla_\theta f_\theta(x,y_w,y_l) \big],
\end{equation}
where
\begin{equation}
    \nabla_\theta f_\theta = \beta \nabla_\theta \log \pi_\theta(y_w|x) - \beta \nabla_\theta \log \pi_\theta(y_l|x).
\end{equation}
Thus, when the model underestimates the preferred trajectory (small
$\pi_\theta(y_w|x)$) or overestimates the hallucinated trajectory (large
$\pi_\theta(y_l|x)$), $f_\theta$ becomes small and $1-\sigma(f_\theta)$ is
large, amplifying the gradient signal. In contrast, standard PPO with a binary
reward only scales the gradient by a scalar advantage term and cannot easily
distinguish between ``slightly bad'' and ``catastrophically hallucinated''
trajectories.

In our framework, IPR deliberately constructs \emph{hard negatives} $y_l$:
they are not arbitrary low-quality samples, but concrete hallucinated
perceptions that initially failed validation and were later corrected by the
same model. This yields preference pairs where $y_w$ and $y_l$ are close in
surface form but sharply different in visual grounding. The SR-DPO gradient
therefore focuses on precisely the directions in parameter space that reduce
hallucinations, rather than pushing the model away from generic failures.

\subsubsection{Gradient Geometry and Hard Negatives}
To further understand the behavior of SR-DPO, we analyze the magnitude of its
gradient as a function of the probability ratio between preferred and rejected
samples. For simplicity, consider the scalar variable
\begin{equation}
    \Delta = \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)},
\end{equation}
and ignore the reference model for exposition. The SR-DPO loss on a single
pair reduces to $\ell(\Delta) = -\log \sigma(\beta \Delta)$, whose derivative
is
\begin{equation}
    \frac{\partial \ell}{\partial \Delta} = -\beta \big(1-\sigma(\beta \Delta)\big).
\end{equation}
Thus, the gradient magnitude is largest when $\Delta$ is small or negative
(i.e., the model prefers the hallucinated sample), and decays smoothly as the
model correctly ranks $y_w$ above $y_l$. In contrast, binary cross-entropy
(standard SFT) on $y_w$ only depends on $\pi_\theta(y_w|x)$ and ignores
$y_l$, while PPO scales its gradient by an advantage term that is often noisy
and insensitive to the fine-grained relative ordering of multiple candidates.

This geometry can be visualized by plotting $\lVert \nabla_\theta \ell \rVert$
as a function of $(\pi_\theta(y_w|x), \pi_\theta(y_l|x))$: SR-DPO forms
"ridges" of large gradient where the model assigns comparable or higher
probability to $y_l$ than to $y_w$. BCE concentrates mass near regions where
$\pi_\theta(y_w|x)$ is extremely small, and PPO's gradient surface is further
modulated by the clipped importance ratio and value baseline, which can
collapse for out-of-distribution negatives.

The role of IPR-generated hard negatives can be formalized from an
information-theoretic viewpoint. Let $D_{\text{KL}}$ denote the KL divergence
between the current policy and an ideal hallucination-free policy
$\pi^*$. Preference pairs induced by random negatives tend to have small
log-probability gaps $\Delta$ that are already correctly ordered, leading to
weak gradients and slow decrease of $D_{\text{KL}}(\pi_\theta\Vert\pi^*)$ per
update. In contrast, hard negatives are constructed such that
$y_l$ lies in regions where $\pi_\theta$ assigns spuriously high probability
while $\pi^*$ assigns low probability (e.g., visually unsupported yet
plausible descriptions). Optimizing SR-DPO on such pairs maximizes the
expected decrease of $D_{\text{KL}}$ per unit gradient norm, since each update
simultaneously increases $\pi_\theta(y_w|x)$ and decreases $\pi_\theta(y_l|x)$
along the most misaligned directions.

\subsubsection{Variance Reduction Analysis}
We now make the above intuition explicit by analyzing the variance of the
stochastic gradient induced by different negative sampling strategies.

\paragraph{Stochastic gradient of SR-DPO.}
For one preference triple $(x,y_w,y_l)$, define the per-sample loss
$\ell(x,y_w,y_l) = -\log \sigma\big(f_\theta(x,y_w,y_l)\big)$. Its stochastic
gradient is
\begin{equation}
\begin{aligned}
    g_{SR}(\theta; x,y_w,y_l)
    &\triangleq \nabla_\theta \ell(x,y_w,y_l)\\
    &= -\big(1-\sigma(f_\theta)\big)\,\beta\Big(\nabla_\theta \log\pi_\theta(y_w|x)\\
    &\qquad\qquad\qquad\quad -\nabla_\theta\log\pi_\theta(y_l|x)\Big).
\end{aligned}
\end{equation}
For a mini-batch of size $B$ with i.i.d. samples, the empirical gradient is
$\widehat{g}=\frac{1}{B}\sum_{i=1}^B g_i$, hence
\begin{equation}
    \mathrm{Var}[\widehat{g}] = \tfrac{1}{B}\,\mathrm{Var}[g].
\end{equation}

\paragraph{Random-DPO vs. SR-DPO as conditional variance reduction.}
Assume both methods share the same marginal distribution over contexts and
preferred perceptions $(x,y_w)$, and differ only in how the rejected sample
$y_l$ is drawn:
\begin{equation}
\begin{aligned}
    y_l &\sim q_{\text{rand}}(\cdot\mid x,y_w) &&\text{(Random-DPO)},\\
    y_l &\sim q_{\text{sr}}(\cdot\mid x,y_w)   &&\text{(SR-DPO via IPR)}.
\end{aligned}
\end{equation}
In particular, IPR produces \emph{on-policy hard negatives} from the model's
own failure trajectories, so $q_{\text{sr}}(\cdot\mid x,y_w)$ is typically
more concentrated (often close to deterministic) than broad random sampling
$q_{\text{rand}}$.

Let $g(\theta;x,y_w,y_l)$ denote the per-sample gradient (the expression above
applies to both, but under different $q$). By the law of total variance,
\begin{equation}
\label{eq:total_variance_dpo}
\begin{aligned}
    \mathrm{Var}[g]
    &= \mathbb{E}_{x,y_w}\Big[\mathrm{Var}_{y_l\sim q(\cdot\mid x,y_w)}\big(g\mid x,y_w\big)\Big]\\
    &\quad + \mathrm{Var}_{x,y_w}\Big(\mathbb{E}_{y_l\sim q(\cdot\mid x,y_w)}\big[g\mid x,y_w\big]\Big).
\end{aligned}
\end{equation}
For any fixed $(x,y_w)$, if $q_{\text{sr}}(\cdot\mid x,y_w)$ is a mean-preserving
contraction of $q_{\text{rand}}(\cdot\mid x,y_w)$ (i.e., it puts probability
mass on fewer, more similar rejected candidates), then
\begin{equation}
\label{eq:cond_var_reduction}
    \mathrm{Var}_{y_l\sim q_{\text{sr}}}\big(g\mid x,y_w\big)
    \le \mathrm{Var}_{y_l\sim q_{\text{rand}}}\big(g\mid x,y_w\big).
\end{equation}
Combining \eqref{eq:total_variance_dpo} and \eqref{eq:cond_var_reduction} gives
\begin{equation}
\label{eq:var_ineq_sr_vs_rand}
    \mathrm{Var}\big[\nabla\mathcal{L}_{SR\text{-}DPO}\big]
    \le \mathrm{Var}\big[\nabla\mathcal{L}_{\text{Random-DPO}}\big],
\end{equation}
with a \emph{strict} inequality whenever there exists a non-negligible set of
contexts $(x,y_w)$ such that (i) $q_{\text{rand}}(\cdot\mid x,y_w)$ assigns
non-zero probability to at least two rejected candidates yielding different
gradients, and (ii) $q_{\text{sr}}(\cdot\mid x,y_w)$ is more concentrated.

\paragraph{Connection to faster and stabler convergence.}
Eq.~\eqref{eq:var_ineq_sr_vs_rand} implies that, for the same batch size $B$,
SR-DPO yields a lower-variance gradient estimator than Random-DPO. This higher
signal-to-noise ratio stabilizes updates and empirically accelerates
optimization, which matches our observation that on-policy hard negatives
produce more consistent improvements in visual grounding.

\paragraph{Comparison to PPO (binary rewards).}
For PPO, the policy-gradient estimator typically takes the form
$g_{PPO}(\theta;x,y)=\widehat{A}(x,y)\,\nabla_\theta\log\pi_\theta(y|x)$ (up to
importance-ratio clipping), where $\widehat{A}$ is an advantage estimate built
from stochastic rewards and a learned value baseline. In binary-reward
hallucination settings, $\widehat{A}$ can be high-variance and may change sign
frequently across trajectories, yielding
\begin{equation}
\begin{aligned}
    \mathrm{Var}[g_{PPO}]
    &= \mathbb{E}[\widehat{A}^2\,\lVert\nabla_\theta\log\pi_\theta\rVert^2]
    - \lVert\mathbb{E}[\widehat{A}\,\nabla_\theta\log\pi_\theta]\rVert^2.
\end{aligned}
\end{equation}
In contrast, SR-DPO multiplies the score-difference term by the bounded factor
$\big(1-\sigma(f_\theta)\big)\beta\in[0,\beta]$, which removes the additional
reward/value-estimation noise source and empirically leads to stabler updates.

\noindent \textbf{Why SR-DPO?} \textbf{Stability:} Unlike PPO, which requires hyperparameter-sensitive value networks, DPO is implicit and stable. \textbf{Granularity:} The gradient of DPO scales with the probability ratio, meaning the model receives stronger gradients when it assigns high probability to a ``lazy'' perception $y_l$, effectively pushing it away from language shortcuts. \textbf{On-Policy Alignment:} By constructing preference pairs from the model's own failures (via IPR), SR-DPO directly targets the specific blind spots of the current policy, ensuring that the optimization is strictly relevant to the model's weaknesses.

% -------------------------------------------------------------------
% --- SECTION 4: EXPERIMENTS ---
% -------------------------------------------------------------------
\section{Experiments}\label{sec:experiments}

\begin{table*}[!t]
    \centering
    \caption{Main Results on Multimodal Benchmarks. We report Accuracy (\%) for all benchmarks except MME, for which we report the official score.}\label{tab:main_results}
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabular*}{0.98\textwidth}{@{\extracolsep{\fill}}lccccccc}
        \toprule
        	\textbf{Dataset} &   \textbf{Ours} & \textbf{Qwen2.5-VL-7B} & \textbf{GPT-4V} & \textbf{InternVL3-9B} & \textbf{Llama3.2-90B} & \textbf{Claude3 Opus} & \textbf{DeepSeek-VL2} \\
        \midrule
        \multicolumn{8}{l}{\textit{College-level Problems}} \\
        MMMU$_{\text{val}}$~\cite{ref_mmu}   &   \textbf{62.1} & 58.6 & 61.7 & 59.4 & 60.3 & 54.9 & 54.0 \\
        MMStar~\cite{ref_mmstar}     &   \textbf{66.2} & 64.1 & 56.0 & 67.4 & 55.3 & 45.7 & 61.9 \\
        \midrule
        \multicolumn{8}{l}{\textit{Math}} \\
        MathVista~\cite{ref_mathvista}  &   \textbf{69.8} & 68.1 & 55.2 & 69.0 & 58.2 & 46.1 & 63.9 \\
        \midrule
        \multicolumn{8}{l}{\textit{General Visual Question Answering}} \\
        MMBench\_V11~\cite{ref_mmbench} &   \textbf{83.8} & 82.2 & 79.8 & 82.2 & 77.3 & 59.1 & 81.2 \\
        MMVet~\cite{ref_mmvet}        &   71.2 & 69.7 & 67.5 & \textbf{78.4} & 64.1 & 51.7 & 60.0 \\
        RealWorldQA~\cite{ref_realworldqa}  &   \textbf{70.4} & 68.4 & 68.0 & 71.0 & 68.2 & 48.4 & 70.1 \\
        MME~\cite{ref_mme}          &   \textbf{2342.7} & 2312.1 & 2070.2 & 2368 & 1741 & 1586.8 & 2230.2 \\
        \midrule
        \multicolumn{8}{l}{\textit{Hallucination / Safety}} \\
        HallusionBench~\cite{ref_hallusionbench} &   \textbf{53.4} & 51.9 & 43.9 & 50.8 & 44.1 & 37.8 & 45.3 \\
        POPE~\cite{ref_pope}           &   \textbf{90.2} & 85.9 & 81.8 & 89.6 & 86.3 & 74.0 & - \\
        \bottomrule
    \end{tabular*}
\end{table*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\textwidth,height=0.22\textheight,keepaspectratio]{figures/main_results_accuracy.png}
    \caption{Dataset-wise comparison of model performance on the multimodal benchmark suite. The plot visualizes benchmark-level results for our method and representative baselines, using Accuracy (\%) on all datasets except MME (official score). Our framework exhibits consistently strong performance across diverse task categories, including college-level reasoning, math, general VQA, and hallucination/safety evaluations.}
    \label{fig:main_results_accuracy}
\end{figure*}

\subsection{Experimental Setup}

\subsubsection{Datasets}
We evaluate our method on a diverse suite of multimodal benchmarks covering different capabilities: college-level understanding (\textbf{MMMU$_{\text{val}}$}~\cite{ref_mmu} and \textbf{MMStar}), math reasoning (\textbf{MathVista}), general visual question answering (\textbf{MMBench\_V11}, \textbf{MM-Vet}~\cite{ref_mmvet}, \textbf{RealWorldQA}, and \textbf{MME}), and hallucination/safety (\textbf{HallusionBench} and \textbf{POPE}).

\subsubsection{Implementation Details}
We use \textbf{Qwen-VL-Chat (7B)} as our backbone model.
    	extbf{SFT Stage:} We fine-tune the model for 1 epoch on LLaVA-Instruct-150k to initialize the instruction following capability. \textbf{RL Stage:} We generate preference pairs using IPR on a subset of the training data (50k samples). \textbf{Hyperparameters:} For SR-DPO, we set $\beta=0.1$, learning rate $5e-7$, and batch size 64. The HSR-EE confidence threshold $\tau$ is initialized at 0.7 and learned via REINFORCE.

\subsection{Main Results}

Table \ref{tab:main_results} presents the comparison of our Self-Evolving
Framework against representative strong VLMs on a diverse suite of
multimodal benchmarks. While the table summarizes exact numbers,
Fig.~\ref{fig:radar_overall} provides a holistic view of how these
improvements distribute across different capability dimensions.

    {\bfseries Performance Analysis:}
Our method achieves strong overall performance across a wide range of
benchmarks.Compared with representative strong baselines, the gains are
most pronounced on reasoning- and grounding-sensitive evaluations, supporting
the effectiveness of hierarchical reasoning and preference optimization.

\subsection{Ablation Study}

To understand the contribution of each component, we conduct an ablation study
across three representative benchmarks: MMMU$_{\text{val}}$, MM-Vet, and MathVista.

\begin{table}[!h]
    \centering
    \caption{Ablation study across three benchmarks. We report Accuracy (\%) for all benchmarks.}\label{tab:ablation}
    \footnotesize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{p{3.2cm}ccc}
        	\toprule
        	\textbf{Setting} & \textbf{MMMU$_{\text{val}}$} & \textbf{MM-Vet} & \textbf{MathVista} \\
        \midrule
        	\textbf{Full Framework}         & \textbf{62.1} & \textbf{71.2} & \textbf{69.8} \\
        w/o HSR-EE (Using Static Graph) & 60.5          & 70.0          & 69.3          \\
        w/o IPR (No Refinement)         & 59.2          & 67.9          & 67.4          \\
        w/o SR-DPO (Using Binary PPO)   & 58.4          & 67.1          & 66.7          \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent \textbf{Impact of HSR-EE:} Removing the dynamic hierarchy consistently degrades performance, indicating that adaptive reasoning depth is important for hard, multi-step cases. \textbf{Impact of IPR \& SR-DPO:} These two components are tightly coupled; removing IPR reduces both reasoning and math accuracy, while removing SR-DPO and reverting to Binary PPO causes the largest overall drop, demonstrating that fine-grained preference optimization is crucial for robust gains.

% --- FIGURE 3: QUALITATIVE COMPARISON (MOVED EARLIER) ---
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/figure3.png}
    \caption{Qualitative comparison showing that our framework produces more specific and spatially grounded visual perceptions, reducing vague shortcut answers.}\label{fig:qualitative}
\end{figure}

% \begin{table*}[!t]
%     \centering
%     \caption{Efficiency comparison of fixed-depth baselines and HSR-EE. We report Accuracy (\%) on MMMU$_{\text{val}}$ and MM-Vet, average reasoning steps, and relative token savings compared to the fixed 5-step baseline.}\label{tab:efficiency}
%     \begin{tabular}{lcccc}
%         \toprule
%         	\textbf{Method}     & \textbf{MMMU$_{\text{val}}$} & \textbf{MM-Vet} & \textbf{Avg.\ Steps} & \textbf{Token Saving} \\
%         \midrule
%         Fixed 1-step        & 49.1                & 58.1      & 1.0                  & 80.00\%               \\
%         Fixed 3-step        & 53.2                & 65.1      & 3.0                  & 40.00\%               \\
%         Fixed 5-step        & 61.3                & 69.7      & 5.0                  & 0.00\%                \\
%         \midrule
%         HSR-EE ($\tau=0.9$) & 59.3                & 68.9      & 1.8                  & 64.00\%               \\
%         HSR-EE ($\tau=0.7$) & \textbf{62.1}       & \textbf{71.2} & 2.6              & 48.00\%               \\
%         HSR-EE ($\tau=0.5$) & 60.9                & 70.5      & 3.4                  & 32.00\%               \\
%         \bottomrule
%     \end{tabular}
% \end{table*}

\subsection{Efficiency Analysis of HSR-EE}

To validate the efficiency of the Early Exit mechanism, we analyze the
trade-off between performance and computational cost.
Figure~\ref{fig:efficiency} illustrates the relationship between average
reasoning steps (token consumption) and accuracy on the MM-Vet benchmark.

As shown in Table~\ref{tab:efficiency}, our HSR-EE mechanism with threshold
$\tau=0.7$ achieves the best overall trade-off, reaching \textbf{62.1} on MMMU$_{\text{val}}$
and \textbf{71.2} on MM-Vet while using only 2.6 average reasoning steps.
Compared to the fixed 5-step baseline, this yields a 48\% reduction in token
consumption with improved accuracy.
The key insight is that simple queries can be
resolved in 1--2 steps, while complex queries 
adaptively utilize more steps. This adaptive allocation significantly improves
overall efficiency without sacrificing performance on challenging samples.

% --- TABLE: IPR ITERATIONS ABLATION ---
\begin{table*}[!t]
    \centering
    \caption{Impact of IPR iteration count. HR denotes hallucination rate measured on POPE (lower is better).}
    \begin{tabular}{lccccc}
        \toprule
        	\textbf{Max Iterations} & \textbf{MMMU$_{\text{val}}$} & \textbf{MM-Vet} & \textbf{POPE} & \textbf{HR (\%)} $\downarrow$ & \textbf{Pair Yield} \\
        \midrule
        0 (No IPR)              & 59.4            & 69.7               & 89.1      & 15.9                          & 0\%                 \\
        1                       & 60.8            & 70.8               & 90.1      & 14.1                          & 42.30\%             \\
        	\textbf{2 (Default)}    & \textbf{62.1}   & \textbf{71.2}      & \textbf{90.2} & \textbf{12.6}                 & 58.70\%             \\
        3                       & 61.6            & 71.0               & 89.9      & 13.2                          & 64.20\%             \\
        4                       & 60.9            & 70.7               & 89.5      & 14.6                          & 67.80\%             \\
        5                       & 60.1            & 70.5               & 88.0      & 16.1                          & 69.50\%             \\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Ablation Study on IPR Iterations}

We investigate the impact of the number of refinement attempts in the IPR
module. By default, our framework allows up to 2 refinement iterations. We
extend this analysis to explore whether increasing the iteration budget (3--5
attempts) further improves performance or leads to hallucination accumulation.


Table~\ref{tab:ipr_iterations} reveals several important findings:

\noindent \textbf{Optimal at 2 Iterations:} Performance peaks at 2 refinement attempts, achieving the best MMMU$_{\text{val}}$ (62.1), MM-Vet (71.2), POPE (90.2), and the lowest hallucination rate (12.6\%). \textbf{Diminishing Returns Beyond 2:} While increasing iterations yields more preference pairs (Pair Yield rises from 58.70\% to 69.50\%), performance \textit{degrades}; at 5 iterations, MMMU$_{\text{val}}$ drops to 60.1, POPE drops to 88.0, and HR increases to 16.1\%. \textbf{Hallucination Accumulation:} We hypothesize that excessive refinement attempts cause the model to ``over-imagine'' details, introducing fabricated information that passes self-validation but is factually incorrect; this phenomenon, which we term \textit{refinement hallucination}, occurs when the model hallucinates plausible-sounding details to satisfy the validation criterion. \textbf{Quality vs.\ Quantity Trade-off:} More iterations generate more training pairs, but pair quality degrades, and lower-quality pairs introduce noise into SR-DPO training, ultimately harming generalization.

These results justify our design choice of limiting IPR to 2 iterations,
balancing the benefit of self-correction against the risk of hallucination
accumulation.

\subsection{Qualitative Analysis}
Figure~\ref{fig:qualitative}   illustrates how the refined model avoids
vague prior-based answers. For instance, on chart interpretation tasks, the
baseline yields generic trend statements, while our model grounds numeric and
spatial details after IPR refinement.

% -------------------------------------------------------------------
% --- SECTION 5: CONCLUSION ---
% -------------------------------------------------------------------
\section{Conclusion}\label{sec:conclusion}

In this paper, we identified the structural and objective limitations of
current self-rewarding VLMs. We proposed a Self-Evolving Framework that
transitions from static validation to dynamic evolution. By enabling
Hierarchical Self-Rewarding, we allow the model to think as deeply as
necessary. By implementing Iterative Perception Refinement and SR-DPO, we
create a system that learns specifically from its own mistakes, turning
hallucinations into negative training samples. Our results confirm that this
approach significantly enhances visual grounding and robustness. Future work
will explore applying this framework to video understanding and long-horizon
agentic tasks.

% -------------------------------------------------------------------
% --- REFERENCES ---
% -------------------------------------------------------------------
\bibliographystyle{IEEEtran}

\bibliography{references}

\thispagestyle{empty}

\section*{Author Biography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/ccb.png}}]{Changbin Cheng}
\normalsize
is currently pursuing the B.Eng. degree in Civil Engineering with the School of Civil Engineering and Mechanics, Lanzhou University, Lanzhou, China. His research interests include vision-language models, reinforcement learning, and multimodal reasoning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/djs1.jpg}}]{Jisheng Dang}
\normalsize
received the Ph.D. degree from Sun Yat-sen University, China, in 2025, advised by prof. Jianhuang Lai and prof. Huicheng Zheng. He worked as a research fellow at the NExT++ laboratory of the NationalT University of Singapore advised by prof. Tat-Seng Chua. He is now a tenured associate professor at the School of Information Science and Engineering, Lanzhou University. His research interests include multimodal learning, video understanding, and embodied intelligence. He has published several papers as the first author in major journals and conferences including IEEE TIP/TNNLS/TITS/IJCAI/AAAI. He served as a reviewer at some major journals and conferences like IEEE TPAMI, ICML, NIPS, ICLR, IEEE TIP, CVPR, IJCAI, ACM MM, AAAI, IEEE TMM, IEEE TCSVT, ACM TOMM.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/wbm.jpg}}]{Bimei Wang (Member, IEEE)}
\normalsize
has been pursuing a Ph.D. at the College of Cyber Security, Jinan University, under the supervision of Prof. Jian Weng, a recipient of the National Young Talent Program, since September 2021. Since June 2024, she has been a joint Ph.D. student at the School of Computing, National University of Singapore, under the guidance of Prof. Ee-Chien Chang. Her research interests focus on large language models and their security. As both first author and corresponding author, she has published multiple papers. In addition, she has applied more than Chinese patents as well as one U.S. patents, and has served as a reviewer for prominent international conferences, including AAAI and ICME.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/gyl.png}}]{Yulan Guo (Senior Member, IEEE)}
\normalsize
received the B.E. and Ph.D. degrees from National University of Defense Technology (NUDT) in 2008 and 2015, respectively. He has authored over 150 articles at highly referred journals and conferences. His research interests lie in 3D vision, low-level vision, and machine learning. He served as an area chair for CVPR 2023/2021, ICCV 2021, and ACM Multimedia 2021. He is a Senior Member of IEEE and ACM. He also served as a senior associate editor for IEEE Transactions on Image Processing.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/binhu.png}}]{Bin Hu (Fellow, IEEE)}
\normalsize
received the PhD degree in computer science from the Institute of Computing Technology, Chinese Academy of Science, China, in 1998. Since 2008, he has been a professor and the dean of the School of Information Science and Engineering, Lanzhou University, China. He had been also guest professorship in ETH Zurich, Switzerland till 2011. He is a Professor of Lanzhou University and Beijing Institute of Technology. He serves as Editor-in-Chief of IEEE Transactions on Computational Social Systems, Fellow of IET and AAIA, and Chair of Technical Committee on Computational Psychophysiology, IEEE SMC. He has published over 300 papers in domestic and international academic journals and conferences. His research interests include pervasive computing, computational psychophysiology, data modeling, and artificial intelligence.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/chua.png}}]{Tat-Seng Chua}
\normalsize
received the Ph.D. degree from the University of Leeds, U.K. He is the KITHCT chair professor with the School of Computing, National University of Singapore, where he was the acting and founding dean of the School from 1998 to 2000. He is the co-director of NExT, a joint center between NUS and Tsinghua University, to develop technologies for live social media search. He is the 2015 winner of the prestigious ACM SIGMM Award. He is the chair of Steering Committee of the ACM International Conference on Multimedia Retrieval (ICMR) and Multimedia Modeling (MMM) conference series. He is also the general co-chair of ACM Multimedia 2005, ACM CIVR (now ACM ICMR) 2005, ACM SIGIR 2008, and ACM Web Science 2015. He serves on the editorial boards of four international journals. He is the co-founder of two technology startups in Singapore and a Fellow of the Singapore Academy of Sciences, with   101,866 citations on Google Scholar.
\end{IEEEbiography}


\end{document}
